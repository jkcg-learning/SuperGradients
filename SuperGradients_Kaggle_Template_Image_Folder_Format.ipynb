{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkcg-learning/SuperGradients/blob/main/SuperGradients_Kaggle_Template_Image_Folder_Format.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://github.com/Deci-AI/super-gradients/blob/master/documentation/assets/SG_img/SG%20-%20Horizontal%20Glow.png?raw=true'>\n",
        "\n",
        "\n",
        "## If you want to learn about some fundamental  model architectures, be sure to check out [my FREE courses on Udemy](https://www.udemy.com/user/harpreet-sahota-4/).\n",
        "\n",
        "In this notebook, you'll use the SuperGradients training library to perform image classifcation! \n",
        "\n",
        "[SuperGradients](https://github.com/Deci-AI/super-gradients) is an open-source PyTorch based training library that has a number of pre-trained models for you to use, training recipies that will get you amazing accuracy, and many [training tricks](https://www.deeplearningdaily.community/t/tips-for-training-your-neural-networks/307) that you can use with just the \"flip of a switch\". For this example you'll use an EfficientNetB0 to perfom the classification. You can check out our [model zoo](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/Computer_Vision_Models_Pretrained_Checkpoints.md) and use any of the pretrained models we have available.\n",
        "\n",
        "Feel free to reach out to me on my community forum, [Deep Learning Daily (free and open to all)](https://www.deeplearningdaily.community/), should you have any questions.\n",
        "\n",
        "\n",
        "\n",
        "# Image Classification Project Template with SuperGradients\n",
        "\n",
        "This notebook template is for data that is already in ImageFolder format.\n",
        "\n",
        "If you are unfamiliar with ImageFolder format, it looks like this:\n",
        "\n",
        "```\n",
        "├── train\n",
        "│   ├── class1\n",
        "|      ├── 1.jpg\n",
        "│      ├── 2.jpg\n",
        "│   ├── class2\n",
        "|      ├── 1.jpg\n",
        "│      ├── 2.jpg\n",
        "├── valid\n",
        "│   ├── class1\n",
        "|      ├── 1.jpg\n",
        "│      ├── 2.jpg\n",
        "│   ├── class2\n",
        "|      ├── 1.jpg\n",
        "│      ├── 2.jpg\n",
        "```\n",
        "\n",
        "With parent folder repeating for validtaion and testing data."
      ],
      "metadata": {
        "id": "M_LdvkGuIx9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture \n",
        "# After installation is complete you must restart the notebook otherwise you \n",
        "# will face some import errors\n",
        "!pip install imutils\n",
        "!pip install super_gradients==3.0.7\n",
        "!pip install albumentations \n",
        "!pip install split-folders[full]"
      ],
      "metadata": {
        "id": "4Siqg0CJDbCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pam_bZ2aDYXF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "from typing import Dict, List,Tuple\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from pathlib import Path, PurePath\n",
        "import pathlib\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from imutils import paths\n",
        "\n",
        "import splitfolders\n",
        "import textwrap\n",
        "\n",
        "import super_gradients\n",
        "from super_gradients.common.object_names import Models\n",
        "from super_gradients.training import Trainer\n",
        "from super_gradients.training import training_hyperparams\n",
        "from super_gradients.training.metrics.classification_metrics import Accuracy, Top5\n",
        "from super_gradients.training.utils.early_stopping import EarlyStop\n",
        "from super_gradients.training import models\n",
        "from super_gradients.training.utils.callbacks import Phase"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Number of classes\n",
        "\n",
        "Simple utility to fetch the number of classes."
      ],
      "metadata": {
        "id": "SbT5tUywqnFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def count_subdirectories(path: str) -> int:\n",
        "    \"\"\"\n",
        "    Counts the number of subdirectories in the given directory path.\n",
        "    \"\"\"\n",
        "    dir_path = Path(path)\n",
        "    subdirectories = [f for f in dir_path.iterdir() if f.is_dir()]\n",
        "    return len(subdirectories)\n",
        "\n",
        "# Example usage\n",
        "parent_dir = \"/path/to/parent/directory\"\n",
        "num_subdirectories = count_subdirectories(parent_dir)\n",
        "print(f\"Number of subdirectories in {parent_dir}: {num_subdirectories}\")\n"
      ],
      "metadata": {
        "id": "g0rFIshTqmsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config \n",
        "\n",
        "This holds variables for the notebook.\n",
        "\n",
        "You will define the model, training params, image type, number of classes, and \n",
        "relevant directories in this class.\n",
        "\n",
        "\n",
        "\n",
        "If you have a question you can leave a comment on this notebook, or visit the community and post it in the [Q&A section](https://www.deeplearningdaily.community/c/qanda/8).\n",
        "\n",
        "## Use a different pretrained model\n",
        "\n",
        "You can change the model you use. Take a look at the [SG model zoo](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/Computer_Vision_Models_Pretrained_Checkpoints.md)\n",
        "\n",
        "For example, if you wanted to use RegNet you would do the following:\n",
        "\n",
        "```\n",
        "MODEL_NAME = \"regnetY800\"\n",
        "```\n",
        "\n",
        "Note you can also pass 'model_name=regnetY200', 'model_name=regnetY400', 'model_name=regnetY600' to try a variety of the architecture\n",
        "\n",
        "For ResNet50, you would do:\n",
        "\n",
        "```\n",
        "MODEL_NAME=\"resnet50\" \n",
        "```\n",
        "\n",
        "Note you can also pass 'model_name=resnet18' or 'model_name=resnet34' to try a variety of the architecture\n",
        "\n",
        "For MobileNetV2, you would do:\n",
        "\n",
        "```\n",
        "MODEL_NAME='mobilenet_v2'\n",
        "```\n",
        "\n",
        "For MobileNetV3, you would do:\n",
        "\n",
        "```\n",
        "MODEL_NAME='mobilenet_v3_large'\n",
        "```\n",
        "\n",
        "Note you can also pass 'model_name=mobilenet_v3_small' to try a variety of the architecture\n",
        "\n",
        "\n",
        "For ViT, you would do:\n",
        "\n",
        "\n",
        "```\n",
        "MODEL_NAME='vit_base'\n",
        "```\n",
        "\n",
        "Note you can also pass 'model_name=vit_large' to try a variety of the architecture"
      ],
      "metadata": {
        "id": "gPKlFKEJDfWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class config:\n",
        "    # specify the paths to datasets\n",
        "    ROOT_DIR = Path()\n",
        "    TRAIN_DIR = ROOT_DIR.joinpath('train')\n",
        "    TEST_DIR = ROOT_DIR.joinpath('test')\n",
        "    VAL_DIR = ROOT_DIR.joinpath('val')\n",
        "\n",
        "    # set the input height and width\n",
        "    INPUT_HEIGHT = 224\n",
        "    INPUT_WIDTH = 224\n",
        "\n",
        "    # set the input heig/ht and width\n",
        "    IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "    IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "    \n",
        "    IMAGE_TYPE = '.jpg'\n",
        "    BATCH_SIZE = 128\n",
        "    MODEL_NAME = \n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    TRAINING_PARAMS = 'training_hyperparams/default_train_params'\n",
        "    \n",
        "    NUM_CLASSES = num_subdirectories\n",
        "\n",
        "    \n",
        "    CHECKPOINT_DIR = 'checkpoints'\n"
      ],
      "metadata": {
        "id": "5bsqNIJ7DoO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into training, validation and testing sets.\n",
        "\n",
        "If your data is not already split up into train, validation and test you can use this.\n",
        "\n",
        "Note the `ratio=(.8, .1, .1)` argument is will split it into 80% train, 10% validation, and 10% split. You can adjust thee as you wish. If you only need to split into two folders (say you already have train and validation and just need to split the validation into a test set) then just use two floats instead.\n"
      ],
      "metadata": {
        "id": "z5MYIPWxPMf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitfolders.ratio(config.DOWNLOAD_DIR, output=config.OUTPUT_DIR, seed=42, ratio=(.8, .1, .1), group_prefix=None, move=False)"
      ],
      "metadata": {
        "id": "NtF40M-mPMOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot random images"
      ],
      "metadata": {
        "id": "9o-LvyR0D1AK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_path_list = list(sorted(paths.list_images(config.TRAIN_DIR)))\n",
        "train_image_path_sample = random.sample(population=train_image_path_list, k=20)\n",
        "\n",
        "def examine_images(images:list):\n",
        "    num_images = len(images)\n",
        "    num_rows = int(math.ceil(num_images/5))\n",
        "    num_cols = 5\n",
        "    \n",
        "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(30, 30),tight_layout=True)\n",
        "    axs = axs.ravel()\n",
        "\n",
        "    for i, image_path in enumerate(images[:num_images]):\n",
        "        image = Image.open(image_path)\n",
        "        label = PurePath(image_path).parent.name\n",
        "        axs[i].imshow(image)\n",
        "        axs[i].set_title(f\"Class: {label}\", fontsize=40)\n",
        "        axs[i].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "examine_images(train_image_path_sample)"
      ],
      "metadata": {
        "id": "lMarOo68FmDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wRFogL6BFmrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check for class distribution\n"
      ],
      "metadata": {
        "id": "HsXh-3EgFmzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a list of all subdirectories in the root directory\n",
        "subdirs = [d for d in Path(config.TRAIN_DIR).iterdir() if d.is_dir()]\n",
        "\n",
        "# Initialize a dictionary to store the count of images in each subdirectory\n",
        "image_count = {}\n",
        "\n",
        "# Iterate through each subdirectory\n",
        "for subdir in subdirs:\n",
        "    subdir_images = list(sorted(paths.list_images(subdir)))\n",
        "    image_count[subdir.name] = len(subdir_images)\n",
        "\n",
        "plt.bar(image_count.keys(), image_count.values())\n",
        "# add the count numbers on top of the bars\n",
        "for i, (subdir, count) in enumerate(image_count.items()):\n",
        "    plt.text(i, count + 3, str(count), ha='center')\n",
        "\n",
        "# set the title and labels for the plot\n",
        "plt.title(\"Number of Images in Each Subdirectory\")\n",
        "plt.xlabel(\"Subdirectories\")\n",
        "plt.ylabel(\"Counts\")\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-S_7Ly_mFq4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Augmentations\n"
      ],
      "metadata": {
        "id": "Ic8WXQILKc1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our data augmentation functions\n",
        "resize = transforms.Resize(size=(config.INPUT_HEIGHT,config.INPUT_WIDTH))\n",
        "make_tensor = transforms.ToTensor()\n",
        "normalize = transforms.Normalize(mean=config.IMAGENET_MEAN, std=config.IMAGENET_STD)\n",
        "center_cropper = transforms.CenterCrop((config.INPUT_HEIGHT,config.INPUT_WIDTH))\n",
        "random_horizontal_flip = transforms.RandomHorizontalFlip(p=0.75)\n",
        "random_vertical_flip = transforms.RandomVerticalFlip(p=0.75)\n",
        "random_rotation = transforms.RandomRotation(degrees=90)\n",
        "random_crop = transforms.RandomCrop(size=(200,200))\n",
        "augmix = transforms.AugMix(severity = 3, mixture_width=3, alpha=0.2)\n",
        "auto_augment = transforms.AutoAugment()\n",
        "random_augment = transforms.RandAugment()\n",
        "\n",
        "# initialize our training and validation set data augmentation pipeline\n",
        "train_transforms = transforms.Compose([\n",
        "  resize, \n",
        "  auto_augment,\n",
        "  augmix,\n",
        "  random_augment,\n",
        "  make_tensor,\n",
        "  normalize\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([resize, make_tensor, normalize])"
      ],
      "metadata": {
        "id": "PzQIE96DKdKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Show what one image looks like after augmentation"
      ],
      "metadata": {
        "id": "foobRpfyKfeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_transform(img: Image, transform) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Applies a transform to a PIL Image and returns a numpy array of the transformed image.\n",
        "\n",
        "    Args:\n",
        "        img (PIL.Image): The input image to transform.\n",
        "        transform (torchvision.transforms.Compose): The transform to apply to the image.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A numpy array representing the transformed image.\n",
        "    \"\"\"\n",
        "    # Apply the transform to the image\n",
        "    if isinstance(transform, torchvision.transforms.Compose):\n",
        "        # Apply PyTorch transform to image array\n",
        "        transformed_image = train_transforms(img)\n",
        "\n",
        "    elif isinstance(transform, A.Compose):\n",
        "        # Apply Albumentations transform to image array\n",
        "        img_array = np.array(img)\n",
        "        transformed_image = transform(image=img_array)[\"image\"]\n",
        "\n",
        "    # Convert the image tensor to a numpy array and transpose the axes to (height, width, channels)\n",
        "    img_array = transformed_image.numpy().transpose((1, 2, 0))\n",
        "\n",
        "    # Clip the pixel values to the range [0, 1]\n",
        "    img_array = np.clip(img_array, 0, 1)\n",
        "\n",
        "    return img_array\n",
        "\n",
        "\n",
        "def visualize_transform(image: np.ndarray, original_image: np.ndarray = None) -> None:\n",
        "    \"\"\"\n",
        "    Visualize the transformed image.\n",
        "\n",
        "    Args:\n",
        "        image (np.ndarray): A NumPy array representing the transformed image.\n",
        "        original_image (np.ndarray, optional): A NumPy array representing the original image. Defaults to None.\n",
        "    \"\"\"\n",
        "    fontsize = 18\n",
        "    \n",
        "    if original_image is None:\n",
        "        # Create a plot with 1 row and 2 columns.\n",
        "        f, ax = plt.subplots(1, 2, figsize=(12, 12))\n",
        "\n",
        "        # Show the transformed image in the first column.\n",
        "        ax[0].imshow(image)\n",
        "    else:\n",
        "        # Create a plot with 1 row and 2 columns.\n",
        "        f, ax = plt.subplots(1, 2, figsize=(12, 12))\n",
        "\n",
        "        # Show the original image in the first column.\n",
        "        ax[0].imshow(original_image)\n",
        "        ax[0].set_title('Original image', fontsize=fontsize)\n",
        "        \n",
        "        # Show the transformed image in the second column.\n",
        "        ax[1].imshow(image)\n",
        "        ax[1].set_title('Transformed image', fontsize=fontsize)\n",
        "        \n",
        "img = Image.open(random.choice(train_image_path_list))\n",
        "img_array = apply_transform(img, train_transforms)\n",
        "visualize_transform(img_array, original_image=img)"
      ],
      "metadata": {
        "id": "LFNBjz6JKfkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets and Dataloadrer"
      ],
      "metadata": {
        "id": "1gVOPxIqFrPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(\n",
        "    train_dir: str, \n",
        "    val_dir: str,\n",
        "    test_dir: str,\n",
        "    train_transform: transforms.Compose,\n",
        "    val_transform:  transforms.Compose,\n",
        "    test_transform:  transforms.Compose,\n",
        "    batch_size: int, \n",
        "    num_workers: int=2\n",
        "):\n",
        "  \"\"\"Creates training and validation DataLoaders.\n",
        "  Args:\n",
        "    train_dir: Path to training data.\n",
        "    val_dir: Path to validation data.\n",
        "    transform: Transformation pipeline.\n",
        "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
        "    num_workers: An integer for number of workers per DataLoader.\n",
        "  Returns:\n",
        "    A tuple of (train_dataloader, val_dataloader, class_names).\n",
        "  \"\"\"\n",
        "  # Use ImageFolder to create dataset\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "  val_data = datasets.ImageFolder(val_dir, transform=val_transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=val_transform)  \n",
        "\n",
        "  print(f\"[INFO] training dataset contains {len(train_data)} samples...\")\n",
        "  print(f\"[INFO] validation dataset contains {len(val_data)} samples...\")\n",
        "  print(f\"[INFO] test dataset contains {len(test_data)} samples...\")\n",
        "\n",
        "  # Get class names\n",
        "  class_names = train_data.classes\n",
        "  print(f\"[INFO] dataset contains {len(class_names)} labels...\")\n",
        "\n",
        "  # Turn images into data loaders\n",
        "  print(\"[INFO] creating training and validation set dataloaders...\")\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      drop_last=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "      persistent_workers=True\n",
        "  )\n",
        "  val_dataloader = DataLoader(\n",
        "      val_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "      drop_last=False,\n",
        "      persistent_workers=True\n",
        "  )\n",
        "\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True,\n",
        "      drop_last=False,\n",
        "      persistent_workers=True\n",
        "  )\n",
        "\n",
        "  return train_dataloader, val_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "B1aikxZRGCKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader, valid_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=config.TRAIN_DIR,\n",
        "                                                                     val_dir=config.VAL_DIR,\n",
        "                                                                     test_dir=config.TEST_DIR,\n",
        "                                                                     train_transform=train_transforms,\n",
        "                                                                     val_transform=val_transforms,\n",
        "                                                                     test_transform=val_transforms,\n",
        "                                                                     batch_size=config.BATCH_SIZE)\n",
        "\n",
        "NUM_CLASSES = len(class_names)"
      ],
      "metadata": {
        "id": "GWga8jM0GCcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Params\n",
        "\n",
        "Learn more about SuperGradients training params [here](https://github.com/Deci-AI/super-gradients/blob/master/tutorials/what_are_recipes_and_how_to_use.ipynb)\n",
        "\n",
        "There are a few things you can try to see how you fare: try a different optimizer (you can the optimizer by using a passing of the following strings \"Adam\", \"AdamW\", \"SGD\", or \"RMSProp\".\n",
        "\n",
        "You can also enable exponential moving average (change the param for \"ema\" to True), you can enable zero weight decay on bias and batch norm (change the parm for \"zero_weight_decay_on_bias_and_bn\" to True), or you can change the level of label smoothing by playing around with the 'smooth_eps' value.\n",
        "\n",
        "Get a detailed description of all training parameters, what they mean, and their allowable values [here](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/recipes/training_hyperparams/default_train_params.yaml)."
      ],
      "metadata": {
        "id": "GG6RomNeGCnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_params =  training_hyperparams.get(config.TRAINING_PARAMS)"
      ],
      "metadata": {
        "id": "Yn5OpeiPGEsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To reduce clutter in the notebook I've turned the verbosity off, you can turn it on to see the full output\n",
        "early_stop_acc = EarlyStop(Phase.VALIDATION_EPOCH_END, monitor=\"Accuracy\", mode=\"max\", patience=7, verbose=False)\n",
        "early_stop_val_loss = EarlyStop(Phase.VALIDATION_EPOCH_END, monitor=\"LabelSmoothingCrossEntropyLoss\", mode=\"min\", patience=7, verbose=False)\n",
        "\n",
        "training_params[\"train_metrics_list\"] = [Accuracy(), Top5()]\n",
        "training_params[\"valid_metrics_list\"] = [Accuracy(), Top5()]\n",
        "training_params[\"phase_callbacks\"] = [early_stop_acc, early_stop_val_loss]\n",
        "\n",
        "# Set the silent mode to True to reduce clutter in the notebook, you can turn it on to see the full output\n",
        "training_params[\"silent_mode\"] = False\n",
        "# We'll turn off the use of exponential moving average and zero weight decay on bias and batch norm\n",
        "training_params['zero_weight_decay_on_bias_and_bn'] = False\n",
        "training_params[\"optimizer\"] = 'Adam'\n",
        "training_params[\"criterion_params\"] = {'smooth_eps': 0.20}\n",
        "training_params[\"max_epochs\"] = 250\n",
        "training_params[\"initial_lr\"] = 0.0001"
      ],
      "metadata": {
        "id": "Rp_9VlAB32Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get model\n",
        "\n"
      ],
      "metadata": {
        "id": "Glmd2R98GZmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.get(config.MODEL_NAME, num_classes = config.NUM_CLASSES, pretrained_weights='imagenet')"
      ],
      "metadata": {
        "id": "rht8gYzaGFFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate trainer"
      ],
      "metadata": {
        "id": "ZliLB58FGFYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_model_trainer = Trainer(experiment_name='0_Baseline_Experiment', ckpt_root_dir=config.CHECKPOINT_DIR)"
      ],
      "metadata": {
        "id": "uBGak3EPGHVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "7zwKpRYiGcIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_model_trainer.train(model=model, \n",
        "              training_params=training_params, \n",
        "              train_loader=train_dataloader,\n",
        "              valid_loader=valid_dataloader)"
      ],
      "metadata": {
        "id": "VaavBbnFGcat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get best model"
      ],
      "metadata": {
        "id": "OtDlvFYp4OAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_full_model = models.get(config.MODEL_NAME,\n",
        "                        num_classes=config.NUM_CLASSES,\n",
        "                        checkpoint_path=os.path.join(full_model_trainer.checkpoints_dir_path, \"ckpt_best.pth\"))\n",
        "\n",
        "# note if you're averaging the best model then replace \"ckpt_best.pth\" with \"average_model.pth\""
      ],
      "metadata": {
        "id": "sguIHkpa4Og3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate on test set"
      ],
      "metadata": {
        "id": "u-n1DyypGkjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_model_trainer.test(model=best_full_model,\n",
        "            test_loader=test_dataloader,\n",
        "            test_metrics_list=['Accuracy', 'Top5'])"
      ],
      "metadata": {
        "id": "Z8H6camVGmY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot predictions"
      ],
      "metadata": {
        "id": "6aNH0FiQGmvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "def pred_and_plot_image(image_path: str, \n",
        "                        subplot: Tuple[int, int, int],  # subplot tuple for `subplot()` function\n",
        "                        class_names: List[str] = class_names,\n",
        "                        model: torch.nn.Module = best_full_model,\n",
        "                        image_size: Tuple[int, int] = (config.INPUT_HEIGHT, config.INPUT_WIDTH),\n",
        "                        transform: torchvision.transforms = None,\n",
        "                        device: torch.device=config.DEVICE):\n",
        "\n",
        "    if isinstance(image_path, pathlib.PosixPath):\n",
        "      img = Image.open(image_path)\n",
        "    else: \n",
        "      img = Image.open(requests.get(image_path, stream=True).raw)\n",
        "\n",
        "    # create transformation for image (if one doesn't exist)\n",
        "    if transform is None:\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=config.IMAGENET_MEAN,\n",
        "                                 std=config.IMAGENET_STD),\n",
        "        ])\n",
        "    transformed_image = transform(img)\n",
        "\n",
        "    # make sure the model is on the target device\n",
        "    model.to(device)\n",
        "\n",
        "    # turn on model evaluation mode and inference mode\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "        # add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
        "        transformed_image = transformed_image.unsqueeze(dim=0)\n",
        "\n",
        "        # make a prediction on image with an extra dimension and send it to the target device\n",
        "        target_image_pred = model(transformed_image.to(device))\n",
        "\n",
        "    # convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "    # convert prediction probabilities -> prediction labels\n",
        "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "    # actual label\n",
        "    ground_truth = PurePath(image_path).parent.name\n",
        "\n",
        "    # plot image with predicted label and probability \n",
        "    plt.subplot(*subplot)\n",
        "    plt.imshow(img)\n",
        "    if isinstance(image_path, pathlib.PosixPath):\n",
        "        title = f\"Ground Truth: {ground_truth} | Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n",
        "    else:\n",
        "        title = f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n",
        "    plt.title(\"\\n\".join(textwrap.wrap(title, width=20)))  # wrap text using textwrap.wrap() function\n",
        "    plt.axis(False)\n",
        "    \n",
        "\n",
        "def plot_random_test_images(model):\n",
        "    num_images_to_plot = 30\n",
        "    test_image_path_list = [pathlib.PosixPath(p) for p in sorted(paths.list_images(config.TEST_DIR))] # get list all image paths from test data \n",
        "    test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
        "                                           k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
        "\n",
        "    # set up subplots\n",
        "    num_rows = int(np.ceil(num_images_to_plot / 5))\n",
        "    fig, ax = plt.subplots(num_rows, 5, figsize=(15, num_rows * 3))\n",
        "    ax = ax.flatten()\n",
        "\n",
        "    # Make predictions on and plot the images\n",
        "    for i, image_path in enumerate(test_image_path_sample):\n",
        "        pred_and_plot_image(model=model, \n",
        "                            image_path=image_path,\n",
        "                            class_names=class_names,\n",
        "                            subplot=(num_rows, 5, i+1),  # subplot tuple for `subplot()` function\n",
        "                            image_size=(config.INPUT_HEIGHT, config.INPUT_WIDTH))\n",
        "\n",
        "    # adjust spacing between subplots\n",
        "    plt.subplots_adjust(wspace=1)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "P0pwt5j_G5a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_random_test_images(best_full_model)\n"
      ],
      "metadata": {
        "id": "ioMk-avcMDre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict on images from internet"
      ],
      "metadata": {
        "id": "hGl1WHkXG0I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = \n",
        "pred_and_plot_image(image_path= image_url, subplot=(1, 1, 1))"
      ],
      "metadata": {
        "id": "NhaOIe_34hdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion matrix"
      ],
      "metadata": {
        "id": "G1oT2vCm4nG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Set model to evaluation mode\n",
        "best_full_model.eval()\n",
        "\n",
        "# Create empty lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Loop over batches in test dataloader, make predictions, and append true and predicted labels to lists\n",
        "for images, labels in test_dataloader:\n",
        "    images = images.to(config.DEVICE)\n",
        "    labels = labels.to(config.DEVICE)\n",
        "    with torch.no_grad():\n",
        "        outputs = best_full_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "    true_labels.extend(labels.cpu().numpy())\n",
        "    predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate confusion matrix, precision, and recall\n",
        "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
        "\n",
        "# Create figure and axis objects with larger size and font size\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "plt.rcParams.update({'font.size': 16})\n",
        "\n",
        "# Create heatmap of confusion matrix\n",
        "im = ax.imshow(conf_matrix, cmap='Blues')\n",
        "\n",
        "# Add colorbar to heatmap\n",
        "cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "# Set tick labels and axis labels with larger font size\n",
        "ax.set_xticks(np.arange(len(class_names)))\n",
        "ax.set_yticks(np.arange(len(class_names)))\n",
        "ax.set_xticklabels(class_names, fontsize=14)\n",
        "ax.set_yticklabels(class_names, fontsize=14)\n",
        "ax.set_xlabel('Predicted label', fontsize=16)\n",
        "ax.set_ylabel('True label', fontsize=16)\n",
        "\n",
        "# Rotate tick labels and set alignment\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# Add text annotations to heatmap\n",
        "for i in range(len(class_names)):\n",
        "    for j in range(len(class_names)):\n",
        "        if conf_matrix[i, j] >= -1:  # Modify threshold value as needed\n",
        "            text = ax.text(j, i, conf_matrix[i, j],\n",
        "                           ha=\"center\", va=\"center\", color=\"y\", fontsize=16)\n",
        "        else:\n",
        "            text = ax.text(j, i, \"\",\n",
        "                           ha=\"center\", va=\"center\", color=\"y\")\n",
        "\n",
        "# Add title to plot with larger font size\n",
        "ax.set_title(\"Confusion matrix\", fontsize=20)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DFidVkd-4p2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your homework\n",
        "\n",
        "Copy/fork this notebook and try some different architectures.\n",
        "\n",
        "If you have a question you can leave a comment on this notebook, or visit the community and post it in the [Q&A section](https://www.deeplearningdaily.community/c/qanda/8).\n",
        "\n",
        "## Use a different pretrained model\n",
        "\n",
        "You can change the model you use. Take a look at the [SG model zoo](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/training/Computer_Vision_Models_Pretrained_Checkpoints.md)\n",
        "\n",
        "For example, if you wanted to use RegNet you would do the following:\n",
        "\n",
        "```\n",
        "resnet_imagenet_model = models.get(model_name='regnetY800', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\n",
        "resnet_params =  training_hyperparams.get('training_hyperparams/imagenet_regnetY_train_params')\n",
        "```\n",
        "\n",
        "Note you can also pass 'model_name=regnetY200', 'model_name=regnetY400', 'model_name=regnetY600' to try a variety of the architecture\n",
        "\n",
        "For ResNet50, you would do:\n",
        "\n",
        "```\n",
        "resnet_imagenet_model = models.get(model_name='resnet50', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\n",
        "resnet_params =  training_hyperparams.get('training_hyperparams/imagenet_resnet50_train_params')\n",
        "```\n",
        "\n",
        "Note you can also pass 'model_name=resnet18' or 'model_name=resnet34' to try a variety of the architecture\n",
        "\n",
        "For MobileNetV2, you would do:\n",
        "\n",
        "```\n",
        "mobilenet_imagenet_model = models.get(model_name='mobilenet_v2', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\n",
        "resnet_params =  training_hyperparams.get('training_hyperparams/imagenet_mobilenetv2_train_params')\n",
        "```\n",
        "\n",
        "For MobileNetV3, you would do:\n",
        "\n",
        "```\n",
        "mobilenet_imagenet_model = models.get(model_name='mobilenet_v3_large', num_classes=NUM_CLASSES, pretrained_weights='imagenet)\n",
        "resnet_params =  training_hyperparams.get('training_hyperparams/imagenet_mobilenetv3_train_params')\n",
        "```\n",
        "\n",
        "Note you can also pass 'model_name=mobilenet_v3_small' to try a variety of the architecture\n",
        "\n",
        "\n",
        "For ViT, you would do:\n",
        "\n",
        "\n",
        "```\n",
        "vit_imagenet_model = models.get(model_name='vit_base', num_classes=NUM_CLASSES, pretrained_weights='imagenet')\n",
        "vit_params =  training_hyperparams.get(\"training_hyperparams/imagenet_vit_train_params\")\n",
        "```\n",
        "\n",
        "Note you can also pass 'model_name=vit_large' to try a variety of the architecture\n",
        "\n",
        "\n",
        "I encourage you play around with different optimizers, all you have to do is change the value of `training_params[\"optimizer\"]`. You can use one of ['Adam','SGD','RMSProp'] out of the box. You can play around with the optimizer params as well.\n",
        "\n",
        "In general, play and tweak around the training recipies...\n",
        "\n",
        "## Training recipes\n",
        "\n",
        "SuperGradients has a number of [training recipes](https://github.com/Deci-AI/super-gradients/tree/master/src/super_gradients/recipes) you can use. [See here](https://github.com/Deci-AI/super-gradients/blob/master/src/super_gradients/recipes/training_hyperparams/default_train_params.yaml) for more information about the training params.\n",
        "\n",
        "If you're using Weights and Biases to track your experiments, you would do the following\n",
        "\n",
        "```\n",
        "sg_logger: wandb_sg_logger\n",
        "sg_logger_params:\n",
        "project_name: <YOUR PROJECT NAME>\n",
        "entity: algo\n",
        "api_server: https://wandb.research.deci.ai\n",
        "save_checkpoints_remote: True\n",
        "save_tensorboard_remote: True\n",
        "save_logs_remote: True\n",
        "```"
      ],
      "metadata": {
        "id": "grHxgV51Jv_G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9XOwNez_Jzzn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}